import urllib.request, urllib.error, urllib.parse
import json
import path
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import time
import requests
import fuckCaptcha

import random
import http.client
import socket

def urlremain(url):
    try:
        req = urllib.request.Request(url)
        response = urllib.request.urlopen(req)
        return True
    except urllib.error.URLError:
        return False

def getWotScore(url):
    ulrwot='http://api.mywot.com/0.4/public_link_json2?hosts='+url+'&key=373fe6c5f2a19ef6841fb3965fd06f480d975e1e'
    req = urllib.request.Request(ulrwot)
    response = urllib.request.urlopen(req).read().decode('utf-8')
    return response


# Json = json.loads(getWotScore('http://bit.ly/1HXqx4O'))
# print(Json['bit.ly']['0'][0])


# proto, rest = urllib.parse("docs.python.org/dsds")
# res, rest = urllib.splithost(rest)
# print ("unkonw" if not res else res)

UlrWot=dict()
listofproxy=[]
with open(path.datapath+'proxylist.txt',mode='r') as pro:
    for line in pro:
        listofproxy.append(json.loads(line.replace("\n",'')))

with open(path.datapath+'/webpagefortwitter/Tweet_JSON/'+'URLsRumors.txt', mode='r')as Seenlist2:
     for line in Seenlist2:
         line=line.replace("\n",'')
         UlrWot[line]=0

with open(path.datapath+'/webpagefortwitter/Tweet_JSON/'+'URLs.txt', mode='r')as Seenlist2:
     for line in Seenlist2:
         line=line.replace("\n",'')
         UlrWot[line]=0
with open(path.datapath+'/webpagefortwitter/Tweet_JSON/'+'URLsBBC.txt', mode='r')as Seenlist2:
     for line in Seenlist2:
         line=line.replace("\n",'')
         UlrWot[line]=0

with open(path.datapath+'/webpagefortwitter/Tweet_JSON/'+'WOtURLs.txt', mode='r')as Seenlist2:
     for line in Seenlist2:
         line=line.replace("\n",'').split("   ")
         UlrWot[line[0]]=line[1]
def getHeader():
    ua = UserAgent()
    header={'User-agent': ua.random,'DNT': "1",'Accept-Language': "en-US;q=0.8,en;q=0.2",'Accept-Encoding': "deflate, sdch"}
    return header    

            # proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()
            #
            # opener = urllib.request.build_opener(proxy_handler, proxy_auth_handler)
            # opener.addheaders =[('User-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36'),('DNT', "1"),('Accept-Language', "en-US;q=0.8,en;q=0.2"),('Accept-Encoding', "gzip, deflate, sdch")]
            # opener.open('http://www.google.com').read()
            #res = conn.getresponse()
       # except socket.gaierror:


#proxies=getProxy()
def getRanknoproxy(url):
    #opener = urllib.request.build_opener()
    # header=getHeader()
    # if len(header) ==0:
    #     header=getHeader()
    #proxies=getProxy()
    try:
        data=requests.get('http://www.alexa.com/siteinfo/'+url).text
        return phaseAlexa(data)
    except Exception:
        print('getRank error')

        return 'error'

def getRank(url,proxies):
    #opener = urllib.request.build_opener()
    # header=getHeader()
    # if len(header) ==0:
    #     header=getHeader()
    #proxies=getProxy()
    try:
        data=requests.get('http://www.alexa.com/siteinfo/'+url,proxies=proxies).text
        return phaseAlexa(data)
    except Exception:
        print('getRank error')

        return 'error'
    #data=opener.open('http://www.alexa.com/siteinfo/'+url).read()
    #data=data.decode('utf-8')

#16,852,849"
def getNum(numstring):
    numberstr='0123456789.'
    tempstr=numstring
    for char in numstring:
        if char not in numberstr:
            tempstr=tempstr.replace(char,'')
    return int(tempstr)
def phaseAlexa(html):
    css_soup = BeautifulSoup(html)

    #css_soup.select(".align-vmiddle")
    rank= css_soup.select(".change-r2")[0].parent.strong.text.replace('\n','')
    try:
        return getNum(rank)
    except ValueError:
        return 20000000


def getTypenoproxy(url,captcha=None):
    #proxies=getProxy()
    try:
        header=getHeader()
        r=None
        url=str("http://sitereview.bluecoat.com/rest/captcha.jpg?1470834655096")
        result= fuckCaptcha.distinguish_captcha(url)

        if captcha==None:
            r = requests.post('http://sitereview.bluecoat.com/rest/categorization', data = {'url':url},headers=header,timeout=4)
        else:
            print(("get type with captcha"+str(captcha)))
            r = requests.post('http://sitereview.bluecoat.com/rest/categorization', data = {'url':url,'captcha':captcha},headers=header,timeout=4)
        data=r.json()
        return phaseTypes(url,data)

    except Exception as e:
        print(str(proxy)+str(e))

        return 'error'
def getType(url,proxies):
    #proxies=getProxy()
    try:
        header=getHeader()
        r = requests.post('http://sitereview.bluecoat.com/rest/categorization', data = {'url':url},proxies=proxies,headers=header,timeout=4)
        print(("get type with proxy"+str(proxies)))
        data=r.json()
        return phaseTypes(url,data)

    except Exception:

        # postdata = urllib.parse.urlencode({'url': url})
        # postdata = postdata.encode('utf-8')
        #
        # response = urllib.request.urlopen('http://sitereview.bluecoat.com/rest/categorization',postdata)
        # text =response.read().decode('utf-8')
        # time.sleep('15')
        return 'error'

     #return phaseTypes(url,)
     #return phaseTypes(data)

def phaseTypes(url,html):
    rank ="no type"
    print(html)
    try:
        if not html['unrated']:
            css_soup = BeautifulSoup(html['categorization'])
            #print(html['categorization'])
            #css_soup.select(".align-vmiddle")
            try:
                rank= css_soup.find_all("a")[1].text#.findall('a')
            except IndexError:
                rank= css_soup.find_all("a")[0].text
    except KeyError:
        #r = requests.post('http://sitereview.bluecoat.com/rest/categorization', data = {'url':url},headers=header,timeout=4)
        #print(r.json())
        #getTypenoproxy(url,captcha=result)

        print('phaseTypes error')
        return 'error'
    print(rank)
    return rank
#getType('www.asdasdsafasfaf.com')
listofweb=[]
with open(path.datapath+'/webpagefortwitter/Tweet_JSON/'+'WOtURLs2.txt', mode='r')as Seenlist2:
    for line in Seenlist2:
        url=json.loads(line)['url']
        listofweb.append(url)

blacklistproxy=set()
def getProxy():
        global blacklistproxy
        print('change proxy')
        proxy=None
        while proxy==None:
            try:
                proxy=random.choice(listofproxy)
                while proxy['proto']=='https' or proxy['url'] in blacklistproxy:
                    assert len(listofproxy)>len(blacklistproxy)
                    proxy=random.choice(listofproxy)
                # proxies = {
                #     proxy.split('://')[0]:proxy}
                proxies={proxy['proto']:proxy['url']}
                #print(proxies)
                requests.get('http://www.alexa.com/siteinfo/'+'google.com',proxies=proxies,timeout=4)
                proxy = proxies
            except Exception as e:
                blacklistproxy.add(proxy['url'])
                print((float(len(blacklistproxy))/len(proxy)))
                #print(str(proxy)+str(e))
                return None
        print(('pick:'+str(proxy)))
        return proxy
#'http': 'http://111.13.136.36:80'
#'http': 'http://120.198.248.97:80'
#http://80.112.170.75:80
#'http': 'http://103.27.24.236:83'
#http://123.126.32.102:8080
with open(path.datapath+'/webpagefortwitter/Tweet_JSON/'+'WOtURLs2.txt', mode='a')as Seenlist2:
        wotdict=dict()
        proxies=None
        # while proxies==None:
        #     proxies=getProxy()
        count=0
        for key in list(UlrWot.keys()):
            if key not in listofweb:
                print((str(len(listofweb)+count)+"--------"+str(len(UlrWot))))
                count+=1
                wotdict['type']=getTypenoproxy(key)
                while wotdict['type']=='error':
                    wotdict['type']=getTypenoproxy(key)

                    # blacklistproxy.add(proxies['http'])
                    # proxies=None
                    # while proxies==None:
                    #     proxies=getProxy()
                    # wotdict['type']=getType(key,proxies)
                wotdict['rank']=getRanknoproxy(key)#,proxies)
                while wotdict['rank']=='error':
                    blacklistproxy.add(proxies['http'])
                    proxies=None
                    while proxies==None:
                        proxies=getProxy()
                    wotdict['rank']=getRank(key,proxies)
                wotdict['url']=key
                if UlrWot[key]!=0:
                    Json = json.loads(getWotScore(key+'/'))
                    try:
                        wotdict['wot']=Json[key]['0'][0]
                    except KeyError:
                        wotdict['wot']=0
                else:
                        wotdict['wot']=0

                Seenlist2.write(json.dumps(wotdict)+"\n")


# proxies=None
# while proxies==None:
#     proxies=getProxy()
# print(proxies)
# header=getHeader()
# data=requests.get('http://www.whatsmyip.org/more-info-about-you/',proxies=proxies,headers=header).text
# print(data)